import os
import pickle
import sys
from pathlib import Path

# Add the project root to the Python path
sys.path.append(str(Path(__file__).parent.parent.parent))

# Import configuration
from veritas import (
    MISTRAL_FAISS_INDEX,
    MISTRAL_METADATA_FILE,
    MISTRAL_MODEL_DIR,
    DEFAULT_EMBEDDING_MODEL,
    DEFAULT_TOP_K,
    EMBED_PROMPT,
    MAX_NEW_TOKENS,
    TEMPERATURE,
    ensure_directories,
    resolve_path,
    ensure_parent_dirs
)

import gradio as gr
import torch
import faiss
from transformers import AutoTokenizer, AutoModelForCausalLM
from sentence_transformers import SentenceTransformer

# â”€â”€â”€ Configuration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Retrieval artifacts
INDEX_PATH = resolve_path(MISTRAL_FAISS_INDEX)
META_PATH = resolve_path(MISTRAL_METADATA_FILE)

# Model artifacts
MODEL_PATH = resolve_path(MISTRAL_MODEL_DIR)

# Retrieval settings
TOP_K = DEFAULT_TOP_K

# â”€â”€â”€ Load Retrieval Components â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Ensure directories exist
ensure_directories()
ensure_parent_dirs(INDEX_PATH)
ensure_parent_dirs(META_PATH)

index = faiss.read_index(str(INDEX_PATH))
print(f"[RAG] Index dimensionality: {index.d}")

with open(META_PATH, "rb") as f:
    metas = pickle.load(f)

encoder = SentenceTransformer(DEFAULT_EMBEDDING_MODEL)

# â”€â”€â”€ Load Mistral Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

tokenizer = AutoTokenizer.from_pretrained(str(MODEL_PATH), local_files_only=True)
model     = AutoModelForCausalLM.from_pretrained(
    str(MODEL_PATH),
    local_files_only=True,
    torch_dtype=torch.float32,
)
model.eval()

# â”€â”€â”€ Inference Function with RAG â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def generate_response(user_query: str) -> str:
    # 1. Encode query
    emb_input = EMBED_PROMPT.format(user_query)
    q_emb = encoder.encode(emb_input, normalize_embeddings=True).astype("float32").reshape(1, -1)
    print(f"[RAG] Query embedding shape: {q_emb.shape}")

    # 2. Search index
    distances, indices = index.search(q_emb, TOP_K)
    print("[DEBUG] Indices returned:", indices)
    print("[DEBUG] Distances returned:", distances)

    # 2.a. Inspect the first metadata entry returned
    if indices.shape[1] > 0:
        first = int(indices[0][0])
        print(f"[DEBUG] metas[{first}] keys:", metas[first].keys())
        print(f"[DEBUG] metas[{first}] entry:\n", metas[first])

    # 3. Gather retrieved texts
    retrieved_texts = []
    for idx in indices[0]:
        chunk = metas[idx].get("text") or metas[idx].get("chunk") or ""
        retrieved_texts.append(chunk)

    # 4. DEBUG: show what was retrieved
    context = "\n\n".join(retrieved_texts)
    print("=== Retrieved Context ===\n", context)
    print("=== User Query =======\n", user_query)

    # 5. Build RAG prompt
    combined_prompt = (
        f"Context:\n{context}\n\n"
        f"Question: {user_query}\n"
        f"Answer:"
    )

    # 6. Generate answer
    inputs = tokenizer(combined_prompt, return_tensors="pt")
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=MAX_NEW_TOKENS,
            do_sample=True,
            temperature=TEMPERATURE
        )

    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# â”€â”€â”€ Launch Gradio â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

if __name__ == "__main__":
    gr.Interface(
        fn=generate_response,
        inputs=gr.Textbox(lines=2, placeholder="Ask a questionâ€¦"),
        outputs="text",
        title="ðŸ”® Mistral 7B + RAG Chat",
        description="Locally powered Mistral with FAISS-based retrieval"
    ).launch(share=True)
