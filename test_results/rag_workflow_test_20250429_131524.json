{
  "device": "mps",
  "model_settings": {
    "embedding_model": "sentence-transformers/all-mpnet-base-v2",
    "generation_model": "mistralai/Mistral-7B-Instruct-v0.3",
    "temperature": 0.7,
    "top_p": 0.95,
    "repetition_penalty": 1.1
  },
  "performance_settings": {
    "embed_batch_size": 32,
    "gen_batch_size": 4,
    "embed_max_length": 1024,
    "gen_max_length": 1024,
    "chunk_size": 512
  },
  "results": {
    "chunking": {
      "fixed": {
        "num_chunks": 1,
        "avg_chunk_size": 236.0,
        "chunks": [
          {
            "text": "Retrieval-Augmented Generation (RAG) Systems RAG is a powerful approach that combines large language models with information retrieval to generate more accurate and factual responses. Here are the key components and concepts: Key Components: 1. Document Retriever: Uses dense or sparse embeddings to find relevant documents 2. Context Processor: Chunks and processes documents for efficient retrieval 3. Generator Model: A large language model that generates responses using retrieved context 4. Vector Store: Database for storing and searching document embeddings Chunking in RAG: Chunking is crucial for several reasons: - Breaks large documents into manageable pieces - Enables more precise retrieval of relevant information - Helps maintain context windows within model limits - Improves search accuracy by focusing on specific segments - Allows for better handling of document structure Best Practices: - Use semantic chunking when possible - Maintain appropriate chunk sizes (typically 256-1024 tokens) - Include overlap between chunks to preserve context - Store metadata with chunks for better retrieval - Implement proper preprocessing and cleaning Benefits of RAG: 1. Reduced hallucination through grounding in documents 2. Improved accuracy and factuality of responses 3. Ability to handle domain-specific knowledge 4. Dynamic knowledge updates without model retraining 5. Better transparency and traceability of information Implementation Considerations: - Choose appropriate embedding models - Optimize chunk size for your use case - Consider using hybrid search (dense + sparse) - Implement proper caching mechanisms - Monitor and evaluate retrieval quality",
            "chunk_index": 0,
            "total_chunks": 1,
            "word_count": 236,
            "strategy": "fixed",
            "doc_id": 0
          }
        ]
      },
      "sentence": {
        "num_chunks": 1,
        "avg_chunk_size": 236.0,
        "chunks": [
          {
            "text": "Retrieval-Augmented Generation (RAG) Systems\n\nRAG is a powerful approach that combines large language models with information retrieval to generate more accurate and factual responses. Here are the key components and concepts:\n\nKey Components:\n1. Document Retriever: Uses dense or sparse embeddings to find relevant documents\n2. Context Processor: Chunks and processes documents for efficient retrieval\n3. Generator Model: A large language model that generates responses using retrieved context\n4. Vector Store: Database for storing and searching document embeddings\n\nChunking in RAG:\nChunking is crucial for several reasons:\n- Breaks large documents into manageable pieces\n- Enables more precise retrieval of relevant information\n- Helps maintain context windows within model limits\n- Improves search accuracy by focusing on specific segments\n- Allows for better handling of document structure\n\nBest Practices:\n- Use semantic chunking when possible\n- Maintain appropriate chunk sizes (typically 256-1024 tokens)\n- Include overlap between chunks to preserve context\n- Store metadata with chunks for better retrieval\n- Implement proper preprocessing and cleaning\n\nBenefits of RAG:\n1. Reduced hallucination through grounding in documents\n2. Improved accuracy and factuality of responses\n3. Ability to handle domain-specific knowledge\n4. Dynamic knowledge updates without model retraining\n5. Better transparency and traceability of information\n\nImplementation Considerations:\n- Choose appropriate embedding models\n- Optimize chunk size for your use case\n- Consider using hybrid search (dense + sparse)\n- Implement proper caching mechanisms\n- Monitor and evaluate retrieval quality",
            "chunk_index": 0,
            "total_chunks": 1,
            "word_count": 236,
            "strategy": "sentence",
            "doc_id": 0
          }
        ]
      },
      "paragraph": {
        "num_chunks": 1,
        "avg_chunk_size": 236.0,
        "chunks": [
          {
            "text": "Retrieval-Augmented Generation (RAG) Systems RAG is a powerful approach that combines large language models with information retrieval to generate more accurate and factual responses. Here are the key components and concepts: Key Components: 1. Document Retriever: Uses dense or sparse embeddings to find relevant documents 2. Context Processor: Chunks and processes documents for efficient retrieval 3. Generator Model: A large language model that generates responses using retrieved context 4. Vector Store: Database for storing and searching document embeddings Chunking in RAG: Chunking is crucial for several reasons: - Breaks large documents into manageable pieces - Enables more precise retrieval of relevant information - Helps maintain context windows within model limits - Improves search accuracy by focusing on specific segments - Allows for better handling of document structure Best Practices: - Use semantic chunking when possible - Maintain appropriate chunk sizes (typically 256-1024 tokens) - Include overlap between chunks to preserve context - Store metadata with chunks for better retrieval - Implement proper preprocessing and cleaning Benefits of RAG: 1. Reduced hallucination through grounding in documents 2. Improved accuracy and factuality of responses 3. Ability to handle domain-specific knowledge 4. Dynamic knowledge updates without model retraining 5. Better transparency and traceability of information Implementation Considerations: - Choose appropriate embedding models - Optimize chunk size for your use case - Consider using hybrid search (dense + sparse) - Implement proper caching mechanisms - Monitor and evaluate retrieval quality",
            "chunk_index": 0,
            "total_chunks": 1,
            "word_count": 236,
            "strategy": "paragraph",
            "doc_id": 0
          }
        ]
      },
      "semantic": {
        "num_chunks": 6,
        "avg_chunk_size": 38.666666666666664,
        "chunks": [
          {
            "text": "RAG is a powerful approach that combines large language models with information retrieval to generate more accurate and factual responses. Here are the key components and concepts:",
            "chunk_index": 0,
            "total_chunks": 6,
            "word_count": 27,
            "strategy": "semantic",
            "doc_id": 0
          },
          {
            "text": "Key Components: 1. Document Retriever: Uses dense or sparse embeddings to find relevant documents 2. Context Processor: Chunks and processes documents for efficient retrieval 3. Generator Model: A large language model that generates responses using retrieved context 4. Vector Store: Database for storing and searching document embeddings",
            "chunk_index": 1,
            "total_chunks": 6,
            "word_count": 47,
            "strategy": "semantic",
            "doc_id": 0
          },
          {
            "text": "Chunking in RAG: Chunking is crucial for several reasons: - Breaks large documents into manageable pieces - Enables more precise retrieval of relevant information - Helps maintain context windows within model limits - Improves search accuracy by focusing on specific segments - Allows for better handling of document structure",
            "chunk_index": 2,
            "total_chunks": 6,
            "word_count": 49,
            "strategy": "semantic",
            "doc_id": 0
          },
          {
            "text": "Best Practices: - Use semantic chunking when possible - Maintain appropriate chunk sizes (typically 256-1024 tokens) - Include overlap between chunks to preserve context - Store metadata with chunks for better retrieval - Implement proper preprocessing and cleaning",
            "chunk_index": 3,
            "total_chunks": 6,
            "word_count": 38,
            "strategy": "semantic",
            "doc_id": 0
          },
          {
            "text": "Benefits of RAG: 1. Reduced hallucination through grounding in documents 2. Improved accuracy and factuality of responses 3. Ability to handle domain-specific knowledge 4. Dynamic knowledge updates without model retraining 5. Better transparency and traceability of information",
            "chunk_index": 4,
            "total_chunks": 6,
            "word_count": 37,
            "strategy": "semantic",
            "doc_id": 0
          },
          {
            "text": "Implementation Considerations: - Choose appropriate embedding models - Optimize chunk size for your use case - Consider using hybrid search (dense + sparse) - Implement proper caching mechanisms - Monitor and evaluate retrieval quality",
            "chunk_index": 5,
            "total_chunks": 6,
            "word_count": 34,
            "strategy": "semantic",
            "doc_id": 0
          }
        ]
      },
      "hybrid": {
        "num_chunks": 1,
        "avg_chunk_size": 236.0,
        "chunks": [
          {
            "text": "Retrieval-Augmented Generation (RAG) Systems RAG is a powerful approach that combines large language models with information retrieval to generate more accurate and factual responses. Here are the key components and concepts: Key Components: 1. Document Retriever: Uses dense or sparse embeddings to find relevant documents 2. Context Processor: Chunks and processes documents for efficient retrieval 3. Generator Model: A large language model that generates responses using retrieved context 4. Vector Store: Database for storing and searching document embeddings Chunking in RAG: Chunking is crucial for several reasons: - Breaks large documents into manageable pieces - Enables more precise retrieval of relevant information - Helps maintain context windows within model limits - Improves search accuracy by focusing on specific segments - Allows for better handling of document structure Best Practices: - Use semantic chunking when possible - Maintain appropriate chunk sizes (typically 256-1024 tokens) - Include overlap between chunks to preserve context - Store metadata with chunks for better retrieval - Implement proper preprocessing and cleaning Benefits of RAG: 1. Reduced hallucination through grounding in documents 2. Improved accuracy and factuality of responses 3. Ability to handle domain-specific knowledge 4. Dynamic knowledge updates without model retraining 5. Better transparency and traceability of information Implementation Considerations: - Choose appropriate embedding models - Optimize chunk size for your use case - Consider using hybrid search (dense + sparse) - Implement proper caching mechanisms - Monitor and evaluate retrieval quality",
            "chunk_index": 0,
            "total_chunks": 1,
            "word_count": 236,
            "strategy": "hybrid",
            "doc_id": 0
          }
        ]
      }
    },
    "embedding": {
      "num_embeddings": 1,
      "embedding_dim": 768,
      "duration": 0.09850788116455078,
      "throughput": 10.15147202617796
    },
    "index": {
      "num_vectors": 1,
      "dimension": 768,
      "duration": 0.015882253646850586
    },
    "retrieval": {
      "What are the key components of a RAG system?": {
        "k=1": {
          "num_results": 1,
          "duration": 0.051193952560424805,
          "scores": [
            0.23859724402427673
          ]
        },
        "k=3": {
          "num_results": 1,
          "duration": 0.014539003372192383,
          "scores": [
            0.23859724402427673
          ]
        },
        "k=5": {
          "num_results": 1,
          "duration": 0.010612964630126953,
          "scores": [
            0.23859724402427673
          ]
        },
        "k=10": {
          "num_results": 1,
          "duration": 0.00999307632446289,
          "scores": [
            0.23859724402427673
          ]
        }
      },
      "Why is chunking important in RAG and what are the best practices?": {
        "k=1": {
          "num_results": 1,
          "duration": 0.05948591232299805,
          "scores": [
            0.36003488302230835
          ]
        },
        "k=3": {
          "num_results": 1,
          "duration": 0.010750770568847656,
          "scores": [
            0.36003488302230835
          ]
        },
        "k=5": {
          "num_results": 1,
          "duration": 0.02692723274230957,
          "scores": [
            0.36003488302230835
          ]
        },
        "k=10": {
          "num_results": 1,
          "duration": 0.011204004287719727,
          "scores": [
            0.36003488302230835
          ]
        }
      },
      "What are the main benefits of using RAG for question answering?": {
        "k=1": {
          "num_results": 1,
          "duration": 0.057588815689086914,
          "scores": [
            0.5049260854721069
          ]
        },
        "k=3": {
          "num_results": 1,
          "duration": 0.010675191879272461,
          "scores": [
            0.5049260854721069
          ]
        },
        "k=5": {
          "num_results": 1,
          "duration": 0.010756969451904297,
          "scores": [
            0.5049260854721069
          ]
        },
        "k=10": {
          "num_results": 1,
          "duration": 0.010371208190917969,
          "scores": [
            0.5049260854721069
          ]
        }
      },
      "What implementation considerations should be taken into account when building a RAG system?": {
        "k=1": {
          "num_results": 1,
          "duration": 0.010126829147338867,
          "scores": [
            0.3483436107635498
          ]
        },
        "k=3": {
          "num_results": 1,
          "duration": 0.01093292236328125,
          "scores": [
            0.3483436107635498
          ]
        },
        "k=5": {
          "num_results": 1,
          "duration": 0.010622262954711914,
          "scores": [
            0.3483436107635498
          ]
        },
        "k=10": {
          "num_results": 1,
          "duration": 0.010995149612426758,
          "scores": [
            0.3483436107635498
          ]
        }
      }
    },
    "generation": {
      "What are the key components of a RAG system?": {
        "duration": 9.065616130828857,
        "response_length": 341
      },
      "Why is chunking important in RAG and what are the best practices?": {
        "duration": 15.738513946533203,
        "response_length": 408
      },
      "What are the main benefits of using RAG for question answering?": {
        "duration": 6.884990930557251,
        "response_length": 337
      },
      "What implementation considerations should be taken into account when building a RAG system?": {
        "duration": 18.416364908218384,
        "response_length": 450
      }
    }
  }
}